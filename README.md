Do LLMs Generalize? (Autumn 2024)
============================

## Course staff
Instructor: Ari Holtzman [contact](mailto:ariholtzman@uchicago.edu)

Office hours: 1:30-2:30pm at Searle 213 on Tuesday and Thursdays or by appointment. (I'm out of office 10/8 and 10/24.)

## Logistics

* Location and time: Tuesday and Thursday 3:30-4:50 at Ryerson 177
* [Syllabus](syllabus.md) (Required reading!)
* [Canvas](https://canvas.uchicago.edu/courses/59817)


Schedule
===========================

* Week 1:  Why are LLMs surprising?
	* 10/1 — Introduction
	* 10/3 - [Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://aclanthology.org/2020.acl-main.463/). Emily M. Bender, Alexander Koller. ACL 2020.
	
		[To Dissect an Octopus: Making Sense of the Form/Meaning Debate](https://julianmichael.org/blog/2020/07/23/to-dissect-an-octopus.html). Julian Michael. 2020.
* Week 2: How should we approach LLMs?
	* 10/8 - [Aaron Schein](https://www.aaronschein.com/) guest lecture! 
	
		Discuss: [Measurement in the Age of LLMs: An Application to Ideological Scaling](https://arxiv.org/abs/2312.09203). Sean O'Hagan, Aaron Schein. JoSoCo 2024.
	* 10/10 — [Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?](https://arxiv.org/abs/2308.00189) Ari Holtzman, Peter West, Luke Zettlemoyer. 2023.
* Week 3: Are LLMs distilling more than they're memorizing?
	* 10/15, [Impact of pretraining term frequencies on few-shot numerical reasoning](https://arxiv.org/abs/2202.07206). Yasaman Razeghi, Robert L. Logan IV, Matt Gardner, Sameer Singh. 2022.
	* 10/17, [Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve](https://arxiv.org/abs/2309.13638) [through page 33 is required]. R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L. Griffiths. 2023.
* Week 4: Can LLMs learn what humans _can't_? Can LLMs learn what humans _can_?
	* 10/22, [Mission: Impossible Language Models](https://arxiv.org/abs/2401.06416). Julie Kallini, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, Christopher Potts. ACL 2024.
	* 10/24, [Kanishka Misra](https://kanishka.website/) guest lecture! 
	
		Discuss: [Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing AANNs](https://arxiv.org/abs/2403.19827). Kanishka Misra, Kyle Mahowald. 2024.
* Week 5: Do LLMs generalize backwards? 
	* 10/29, [The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"](https://arxiv.org/abs/2309.12288). Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans. ICLR 2024.
	* 10/31, [Does Refusal Training in LLMs Generalize to the Past Tense?](https://arxiv.org/abs/2407.11969). Maksym Andriushchenko, Nicolas Flammarion. 2024.
* Week 6: What information is extractible from LLM hidden states? What information is _writable to_ LLM hidden states? 
	* 11/5, [Language Models Represent Space and Time](https://arxiv.org/abs/2310.02207). Wes Gurnee, Max Tegmark. ICLR 2024.
	* 11/7, [Linearly Mapping from Image to Text Space](https://arxiv.org/abs/2209.15162). Jack Merullo, Louis Castricato, Carsten Eickhoff, Ellie Pavlick. ICLR 2023.
* Week 7: What would would it mean for an LLM to deceive?
	* 11/12, [Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?](https://arxiv.org/abs/2312.03729). Kevin Liu, Stephen Casper, Dylan Hadfield-Menell, Jacob Andreas. EMNLP 2024.
	* 11/14, [Personas as a Way to Model Truthfulness in Language Models](https://arxiv.org/abs/2310.18168). Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, He He. 2023.
* Week 8: Can we know what LLMs are capable of?
	* 11/19, [Auxiliary task demands mask the capabilities of smaller language models](https://arxiv.org/abs/2404.02418). Jennifer Hu, Michael C. Frank. COLM 2024.

	* 11/21, [Interpretability Illusions in the Generalization of Simplified Models](https://arxiv.org/abs/2312.03656). Dan Friedman, Andrew Lampinen, Lucas Dixon, Danqi Chen, Asma Ghandeharioun. ICML 2024.
* Week 9: Can LLMs be thankful?
    * 11/26, Thanksgiving break!

    * 11/28, Thanksgiving break!
* Week 10: As n->∞, what are LLMs?
	* 12/3, [Do Large Language Models Understand Us?](https://direct.mit.edu/daed/article/151/2/183/110604/Do-Large-Language-Models-Understand-Us) Blaise Agüera y Arcas. Daedalus 2022.
		
		[Language Models as Agent Models](https://arxiv.org/abs/2212.01681). Jacob Andreas. 2022.

	* 12/5, [The Platonic Representation Hypothesis](https://arxiv.org/abs/2405.07987). Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola. 2024.

		[Symbols and grounding in large language models](https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0041). Ellie Pavlick. Philosphical Transactions A.



